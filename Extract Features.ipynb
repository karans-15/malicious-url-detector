{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automatic-yield",
   "metadata": {},
   "source": [
    "# Extracting Features\n",
    "\n",
    "Our Objective is to extract the features from the url so we can use these features further to make predictions and match our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "least-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!c:/users/admin/appdata/local/programs/python/python39/python.exe -m pip install --upgrade pip\n",
    "#!pip install favicon\n",
    "#!pip install tldextract\n",
    "#!pip install python-whois\n",
    "#!pip install lxml\n",
    "#!pip install tldextract\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from subprocess import *\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "import favicon\n",
    "import xml.etree.ElementTree as ET \n",
    "import tldextract\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import whois\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wicked-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decreased-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-04a89bd36e64>:4: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  str_df = df.select_dtypes([np.object])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['having_IP_Address', 'URL_Length', 'Shortining_Service',\n",
      "       'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',\n",
      "       'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',\n",
      "       'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',\n",
      "       'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',\n",
      "       'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',\n",
      "       'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',\n",
      "       'Google_Index', 'Links_pointing_to_page', 'Statistical_report',\n",
      "       'Result'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def getProcessedDataFrame(filepath):\n",
    "    dataset = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(dataset[0])\n",
    "    str_df = df.select_dtypes([np.object]) \n",
    "    str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "\n",
    "    for col in str_df.columns:\n",
    "        str_df[col] = str_df[col].astype(int)\n",
    "    return str_df\n",
    "\n",
    "complete_training = getProcessedDataFrame(\"Training Dataset.arff\")\n",
    "print(complete_training.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-salon",
   "metadata": {},
   "source": [
    "1. Check if url has an IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "involved-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_ip_feature(url):\n",
    "    \n",
    "    #Use regEx to check for IP address \n",
    "    import re\n",
    "    if re.match(r'http://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}/.*', url) != None:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-pressure",
   "metadata": {},
   "source": [
    "2. Check if url is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classical-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_url_feature(url):\n",
    "    \n",
    "    #Checks for long url\n",
    "    if len(url)>75 :\n",
    "        return -1\n",
    "    elif len(url)>=54:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-fancy",
   "metadata": {},
   "source": [
    "3. Check if url is shortened and get the complete ure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_url(shortened_url):\n",
    "    \n",
    "    #Returns expanded url if it is a shortened url\n",
    "    command_stdout = Popen(['curl', shortened_url], stdout=PIPE).communicate()[0]\n",
    "    #print(command_stdout)\n",
    "    output = command_stdout.decode('utf-8')\n",
    "    href_index = output.find(\"href=\")\n",
    "    if href_index == -1:\n",
    "        href_index = output.find(\"HREF=\")\n",
    "    splitted_ = output[href_index:].split('\"')\n",
    "    expanded_url = splitted_[1]\n",
    "    return expanded_url\n",
    "\n",
    "\n",
    "def shortened_url_feature(url):\n",
    "    famous_short_urls = [\"bit.ly\", \"tinyurl.com\", \"goo.gl\",\n",
    "                       \"rebrand.ly\", \"t.co\", \"youtu.be\",\n",
    "                       \"ow.ly\", \"w.wiki\", \"is.gd\"]\n",
    "\n",
    "    domain = url.split(\"://\")[1]\n",
    "    #print(domain)\n",
    "    domain = domain.split(\"/\")[0]\n",
    "    #print(domain)\n",
    "    feature = 1\n",
    "    if domain in famous_short_urls:\n",
    "        feature = -1\n",
    "\n",
    "    complete_url = None\n",
    "    if feature == -1:\n",
    "        complete_url = get_complete_url(url)\n",
    "\n",
    "    return (feature, complete_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-platform",
   "metadata": {},
   "source": [
    "4. Checks if url has '@' in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "average-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_feature(url):\n",
    "    feature = 1\n",
    "    index = url.find(\"@\")\n",
    "    if index!=-1:\n",
    "        feature = -1\n",
    "  \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-constitution",
   "metadata": {},
   "source": [
    "5. Check if url will redirect with '//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect_feature(url):\n",
    "    feature = 1\n",
    "    if url.rindex(\"//\")>7:\n",
    "        feature = -1\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-franklin",
   "metadata": {},
   "source": [
    "6. Check if url has a prefix '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lasting-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_feature(url):\n",
    "    index = url.find(\"://\")\n",
    "    split_url = url[index+3:]\n",
    "    #print(split_url)\n",
    "    index = split_url.find(\"/\")\n",
    "    split_url = split_url[:index]\n",
    "    #print(split_url)\n",
    "    feature = 1\n",
    "    index = split_url.find(\"-\")\n",
    "    #print(index)\n",
    "    if index!=-1:\n",
    "        feature = -1\n",
    "  \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-collector",
   "metadata": {},
   "source": [
    "7. Check if url have sub domains/multi sub domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "applicable-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_domains_feature(url):\n",
    "    \n",
    "    \n",
    "    split_url = url.split(\"://\")[1]\n",
    "    split_url = split_url.split(\"/\")[0]\n",
    "    index = split_url.find(\"www.\")\n",
    "    if index!=-1:\n",
    "        split_url = url[index+4:]\n",
    "    # print(split_url)\n",
    "    index = split_url.rfind(\".\")\n",
    "    # print(index)\n",
    "    if index!=-1:\n",
    "        split_url = split_url[:index]\n",
    "    # print(split_url)\n",
    "    counter = 0\n",
    "    for i in split_url:\n",
    "        if i==\".\":\n",
    "            counter+=1\n",
    "  \n",
    "    label = 1\n",
    "    if counter==2:\n",
    "        label = 0\n",
    "    elif counter >=3:\n",
    "        label = -1\n",
    "  \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-occasion",
   "metadata": {},
   "source": [
    "8. Check if certificate issuer is trustworthy  (Gives phishing in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imposed-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def authority_feature(url):\n",
    "    \n",
    "#     #Check for https\n",
    "#     index_https = url.find(\"https://\")\n",
    "    \n",
    "#     #Check for trustworthy issuer\n",
    "#     valid_auth = [\"GeoTrust\", \"GoDaddy\", \"Network Solutions\", \"Thawte\", \"Comodo\", \"Doster\" , \"VeriSign\", \"LinkedIn\", \"Sectigo\",\n",
    "#                 \"Symantec\", \"DigiCert\", \"Network Solutions\", \"RapidSSLonline\", \"SSL.com\", \"Entrust Datacard\", \"Google\", \"Facebook\"]\n",
    "  \n",
    "#     cmd = \"curl -vvI \" + url\n",
    "\n",
    "#     stdout = Popen(cmd, shell=True, stderr=PIPE, env={}).stderr\n",
    "#     output = stdout.read()\n",
    "#     std_out = output.decode('UTF-8')\n",
    "#     # print(std_out)\n",
    "#     index = std_out.find(\"O=\")\n",
    "\n",
    "#     split = std_out[index+2:]\n",
    "#     index_sp = split.find(\" \")\n",
    "#     cur = split[:index_sp]\n",
    "  \n",
    "#     index_sp = cur.find(\",\")\n",
    "#     if index_sp!=-1:\n",
    "#         cur = cur[:index_sp]\n",
    "#     #print(cur)\n",
    "#     label = -1\n",
    "#     if cur in valid_auth and index_https!=-1:\n",
    "#         label = 1\n",
    "  \n",
    "#     return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-attack",
   "metadata": {},
   "source": [
    "9. Checking Domain registration length > 1 year  (Gives phishing always)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "noble-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def domain_register_len_feature(u):\n",
    "#     extract_res = tldextract.extract(u)\n",
    "#     ul = extract_res.domain + \".\" + extract_res.suffix\n",
    "#     try:\n",
    "#         wres = whois.whois(u)\n",
    "#         f = wres[\"Creation Date\"][0]\n",
    "#         s = wres[\"Registry Expiry Date\"][0]\n",
    "#         if(s>f+relativedelta(months=+12)):\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return -1\n",
    "#     except:\n",
    "#         return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-settle",
   "metadata": {},
   "source": [
    "10. Check if favicon loaded from external domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coral-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def favicon_feature(url):\n",
    "    \n",
    "    try:\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain\n",
    "\n",
    "        favs = favicon.get(url)\n",
    "        # print(favs)\n",
    "        match = 0\n",
    "        for favi in favs:\n",
    "            url2 = favi.url\n",
    "            extract_res = tldextract.extract(url2)\n",
    "            url_ref2 = extract_res.domain\n",
    "\n",
    "        if url_ref in url_ref2:\n",
    "            match += 1\n",
    "\n",
    "        if match >= len(favs)/2:\n",
    "            return 1\n",
    "        return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Not using port feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-ontario",
   "metadata": {},
   "source": [
    "12. Check for https at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "democratic-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_feature(u):\n",
    "    # Assumption - pagename cannot start with this token\n",
    "    ix = u.find(\"//https\")\n",
    "    if(ix==-1):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-mistress",
   "metadata": {},
   "source": [
    "13. Check if less than 22% urls requested from same domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "northern-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_url_feature(url):\n",
    "    extract_res = tldextract.extract(url)\n",
    "    url_ref = extract_res.domain\n",
    "\n",
    "    command_stdout = Popen(['curl', 'https://api.hackertarget.com/pagelinks/?q=' + url], stdout=PIPE).communicate()[0]\n",
    "    links = command_stdout.decode('utf-8').split(\"\\n\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for link in links:\n",
    "        extract_res = tldextract.extract(link)\n",
    "        url_ref2 = extract_res.domain\n",
    "\n",
    "        if url_ref not in url_ref2:\n",
    "            count += 1\n",
    "\n",
    "    count /= len(links)\n",
    "\n",
    "    if count < 0.22:\n",
    "        return 1\n",
    "    elif count < 0.61:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-proceeding",
   "metadata": {},
   "source": [
    "14. Check if anchor URLs less than 31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "political-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_validator(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def url_anchor_feature(url):\n",
    "    try:\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain\n",
    "        html_content = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        a_tags = soup.find_all('a')\n",
    "\n",
    "        if len(a_tags) == 0:\n",
    "            return 1\n",
    "\n",
    "        invalid = ['#', '#content', '#skip', 'JavaScript::void(0)']\n",
    "        bad_count = 0\n",
    "        for t in a_tags:\n",
    "            link = t[\"href\"]\n",
    "\n",
    "            if link in invalid:\n",
    "                bad_count += 1\n",
    "\n",
    "            if url_validator(link):\n",
    "                extract_res = tldextract.extract(link)\n",
    "                url_ref2 = extract_res.domain\n",
    "\n",
    "                if url_ref not in url_ref2:\n",
    "                    bad_count += 1\n",
    "\n",
    "        bad_count /= len(a_tags)\n",
    "\n",
    "        if bad_count < 0.31:\n",
    "            return 1\n",
    "        elif bad_count <= 0.67:\n",
    "            return 0\n",
    "        return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-chicago",
   "metadata": {},
   "source": [
    "15. Check if lot of links in <meta>, <script> and <link> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cooked-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_feature(u):\n",
    "    try:\n",
    "        programhtml = requests.get(u).text\n",
    "        s = BeautifulSoup(programhtml,\"lxml\")\n",
    "        mtags = s.find_all('Meta')\n",
    "        ud = tldextract.extract(u)\n",
    "        upage = ud.domain\n",
    "        mcount = 0\n",
    "        for i in mtags:\n",
    "            u1 = i['href']\n",
    "            currpage = tldextract.extract(u1)\n",
    "            u1page = currpage.domain\n",
    "            if currpage not in ulpage:\n",
    "                mcount+=1\n",
    "        scount = 0\n",
    "        stags = s.find_all('Script')\n",
    "        for j in stags:\n",
    "            u1 = j['href']\n",
    "            currpage = tldextract.extract(u1)\n",
    "            u1page = currpage.domain\n",
    "            if currpage not in u1page:\n",
    "                scount+=1\n",
    "        lcount = 0\n",
    "        ltags = s.find_all('Link')\n",
    "        for k in ltags:\n",
    "            u1 = k['href']\n",
    "            currpage = tldextract.extract(u1)\n",
    "            u1page = currpage.domain\n",
    "            if currpage not in u1page:\n",
    "                lcount+=1\n",
    "        percmtag = 0\n",
    "        percstag = 0\n",
    "        percltag = 0\n",
    "\n",
    "        if len(mtags) != 0:\n",
    "            percmtag = (mcount*100)//len(mtags)\n",
    "        if len(stags) != 0:\n",
    "            percstag = (scount*100)//len(stags)\n",
    "        if len(ltags) != 0:\n",
    "            percltag = (lcount*100)//len(ltags)\n",
    "\n",
    "        if(percmtag+percstag+percltag<17):\n",
    "            return 1\n",
    "        elif(percmtag+percstag+percltag<=81):\n",
    "            return 0\n",
    "        return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-watershed",
   "metadata": {},
   "source": [
    "16. Checks for SFH that contain empty string or about:blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stunning-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfh_feature(u):\n",
    "    try:\n",
    "        programhtml = requests.get(u).text\n",
    "        s = BeautifulSoup(programhtml,\"lxml\")\n",
    "        try:\n",
    "            f = str(s.form)\n",
    "            ac = f.find(\"action\")\n",
    "            if(ac!=-1):\n",
    "                i1 = f[ac:].find(\">\")\n",
    "                u1 = f[ac+8:i1-1]\n",
    "                if(u1==\"\" or u1==\"about:blank\"):\n",
    "                    return -1\n",
    "                er1 = tldextract.extract(u)\n",
    "                upage = erl.domain\n",
    "                erl2 = tldextract.extract(u1)\n",
    "                usfh = erl2.domain\n",
    "                if upage in usfh:\n",
    "                    return 1\n",
    "                return 0\n",
    "            else:\n",
    "                # Check this point\n",
    "                return 1\n",
    "        except:\n",
    "            # Check this point\n",
    "            return 1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-digest",
   "metadata": {},
   "source": [
    " 17. Check if information is being submitted to an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loving-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_email_feature(url):\n",
    "    try:\n",
    "        html_content = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        # Check if no form tag\n",
    "        form_opt = str(soup.form)\n",
    "        idx = form_opt.find(\"mail()\")\n",
    "        if idx == -1:\n",
    "            idx = form_opt.find(\"mailto:\")\n",
    "\n",
    "        if idx == -1:\n",
    "            return 1\n",
    "        return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "muslim-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. Abnormal URL not including in features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-fever",
   "metadata": {},
   "source": [
    "19. Check if url is being redirected more than 1-2 times (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indian-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forwarding_feature(url):\n",
    "#     opt = Popen([\"sh\", \"red.sh\", url], stdout=PIPE).communicate()[0]\n",
    "#     opt = opt.decode('utf-8')\n",
    "#     # print(opt)\n",
    "#     opt = opt.split(\"\\n\")\n",
    "  \n",
    "#     new = []\n",
    "#     for i in opt:\n",
    "#         i = i.replace(\"\\r\", \" \")\n",
    "#         new.extend(i.split(\" \"))\n",
    "  \n",
    "\n",
    "#     count = 0\n",
    "#     for i in new:\n",
    "   \n",
    "#         if i.isdigit():\n",
    "#             conv = int(i)\n",
    "#             if conv > 300 and conv<310:\n",
    "#                 count += 1\n",
    "\n",
    "#     last_url = None\n",
    "#     for i in new[::-1]:\n",
    "#         if url_validator(i):\n",
    "#             last_url = i\n",
    "#             break\n",
    "\n",
    "#     if (count<=1):\n",
    "#         return 1, last_url\n",
    "#     elif count>=2 and count <4:\n",
    "#         return 0, last_url\n",
    "#     return -1, last_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-screen",
   "metadata": {},
   "source": [
    "20. Check if onmouseover changes url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onmouseover_feature(url):\n",
    "    try:\n",
    "        html_content = requests.get(url).text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    if str(soup).lower().find('onmouseover=\"window.status') != -1:\n",
    "        return -1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-reward",
   "metadata": {},
   "source": [
    "21. Check if rightclick is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "herbal-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightclick_feature(url):\n",
    "    try:\n",
    "        html_content = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        if str(soup).lower().find(\"preventdefault()\") != -1:\n",
    "            return -1\n",
    "        elif str(soup).lower().find(\"event.button==2\") != -1:\n",
    "            return -1\n",
    "        elif str(soup).lower().find(\"event.button == 2\") != -1:\n",
    "            return -1\n",
    "        return 1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sought-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. Pop up window feature wont be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-gnome",
   "metadata": {},
   "source": [
    "23. Check for frame border html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "swedish-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iframe_feature(url):\n",
    "    try:\n",
    "        html_content = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        if str(soup.iframe).lower().find(\"frameborder\") == -1:\n",
    "            return 1\n",
    "        return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-walker",
   "metadata": {},
   "source": [
    "24. Check if age of domain is less than 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "complicated-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_of_domain_feature(url):\n",
    "    try:\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "        try:\n",
    "            whois_res = whois.whois(url)\n",
    "            if datetime.datetime.now() > whois_res[\"creation_date\"][0] + relativedelta(months=+6):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-roads",
   "metadata": {},
   "source": [
    "25. Check if DNS Record exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "frozen-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dns_record_feature(url):\n",
    "    try:\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "        try:\n",
    "            whois_res = whois.whois(url)\n",
    "            return 1\n",
    "        except:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-opportunity",
   "metadata": {},
   "source": [
    "26. Check if website traffic < 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "consistent-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_traffic_feature(url):\n",
    "    try:\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "        html_content = requests.get(\"https://www.alexa.com/siteinfo/\" + url_ref).text\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        value = str(soup.find('div', {'class': \"rankmini-rank\"}))[42:].split(\"\\n\")[0].replace(\",\", \"\")\n",
    "\n",
    "        if not value.isdigit():\n",
    "            return -1\n",
    "\n",
    "        value = int(value)\n",
    "        if value < 100000:\n",
    "            return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-princeton",
   "metadata": {},
   "source": [
    "27. Check if page rank < 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "healthy-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_feature(url):\n",
    "    try:\n",
    "        pageRankApi = open('pagerankAPI.txt').readline()\n",
    "        extract_res = tldextract.extract(url)\n",
    "        url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "        headers = {'API-OPR': pageRankApi}\n",
    "        domain = url_ref\n",
    "        req_url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "        request = requests.get(req_url, headers=headers)\n",
    "        result = request.json()\n",
    "        # print(result)\n",
    "        value = result['response'][0]['page_rank_decimal']\n",
    "        if type(value) == str:\n",
    "            value = 0\n",
    "\n",
    "        if value < 2:\n",
    "            return -1\n",
    "        return 1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sitting-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28. Google Index not used \n",
    "#29. Links pointing to page not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-disposal",
   "metadata": {},
   "source": [
    "30. Check if host belongs to top phishing site or not (Not done cause it will need api keys and registration was shut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "formed-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def statistical_report_feature(url):\n",
    "#     phishTankKey = open('/phishTankKey.txt')\n",
    "#     phishTankKey = phishTankKey.readline()[:-1]\n",
    "\n",
    "#     headers = {\n",
    "#         'format': 'json',\n",
    "#         'app_key': phishTankKey,\n",
    "#         }\n",
    "\n",
    "#     def get_url_with_ip(URI):\n",
    "#         \"\"\"Returns url with added URI for request\"\"\"\n",
    "#         url = \"http://checkurl.phishtank.com/checkurl/\"\n",
    "#         new_check_bytes = URI.encode()\n",
    "#         base64_bytes = base64.b64encode(new_check_bytes)\n",
    "#         base64_new_check = base64_bytes.decode('ascii')\n",
    "#         url += base64_new_check\n",
    "#         return url\n",
    "\n",
    "#     def send_the_request_to_phish_tank(url, headers):\n",
    "#         \"\"\"This function sends a request.\"\"\"\n",
    "#         response = requests.request(\"POST\", url=url, headers=headers)\n",
    "#         return response\n",
    "\n",
    "#     url = get_url_with_ip(url)\n",
    "#     r = send_the_request_to_phish_tank(url, headers)\n",
    "\n",
    "#     def parseXML(xmlfile): \n",
    "\n",
    "#         root = ET.fromstring(xmlfile) \n",
    "#         verified = False\n",
    "#         for item in root.iter('verified'): \n",
    "#             if item.text == \"true\":\n",
    "#                 verified = True\n",
    "#                 break\n",
    "\n",
    "#         phishing = False\n",
    "#         if verified:\n",
    "#             for item in root.iter('valid'): \n",
    "#                 if item.text == \"true\":\n",
    "#                     phishing = True\n",
    "#                     break\n",
    "\n",
    "#         return phishing\n",
    "\n",
    "#     inphTank = parseXML(r.text)\n",
    "#     # print(r.text)\n",
    "\n",
    "#     if inphTank:\n",
    "#         return -1\n",
    "#     return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-shopping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "speaking-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "(1, None)\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://tests.mettl.com/test-window-pi?key=GmIzQf0X8KA9tG7fOidIkEbMbqKup9l1Uez7WVPNnfqvFRcQQoVArm5BHZ1hyaVF\"\n",
    "url = \"https://furniture-shop-vanilla-js.netlify.app/\"\n",
    "\n",
    "print(has_ip_feature(url))\n",
    "print(long_url_feature(url))\n",
    "print(shortened_url_feature(url))\n",
    "print(at_feature(url))\n",
    "print(redirect_feature(url))\n",
    "print(prefix_feature(url))\n",
    "print(multi_domains_feature(url))\n",
    "# print(authority_feature(\"https://www.amazon.com\"))\n",
    "# print(domain_register_len_feature(\"https://www.amazon.com\"))  \n",
    "print(favicon_feature(url))\n",
    "print(token_feature(url))\n",
    "print(request_url_feature(url))\n",
    "print(url_anchor_feature(url))\n",
    "print(tags_feature(url))\n",
    "print(sfh_feature(url))\n",
    "print(submit_to_email_feature(url))\n",
    "#print(forwarding_feature(\"https://www.amazon.com\"))\n",
    "print(onmouseover_feature(url))\n",
    "print(rightclick_feature(url))\n",
    "print(iframe_feature(url))\n",
    "print(age_of_domain_feature(url))\n",
    "print(dns_record_feature(url))\n",
    "print(web_traffic_feature(url))\n",
    "print(page_rank_feature(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "recognized-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url):\n",
    "    features_extracted = [0]*21\n",
    "    phStatus, expanded = shortened_url_feature(url)\n",
    "\n",
    "    if expanded is not None:\n",
    "        if len(expanded) >= len(url):\n",
    "            url = expanded\n",
    "\n",
    "    print(url)\n",
    "    features_extracted[0] = has_ip_feature(url)\n",
    "    features_extracted[1] = long_url_feature(url)\n",
    "    features_extracted[2] = phStatus\n",
    "    features_extracted[3] = at_feature(url)\n",
    "    features_extracted[4] = redirect_feature(url)\n",
    "    features_extracted[5] = prefix_feature(url)\n",
    "    features_extracted[6] = multi_domains_feature(url)\n",
    "    features_extracted[7] = favicon_feature(url)\n",
    "    features_extracted[8] = token_feature(url)\n",
    "    features_extracted[9] = request_url_feature(url)\n",
    "    features_extracted[10] = url_anchor_feature(url)\n",
    "    features_extracted[11] = tags_feature(url)\n",
    "    features_extracted[12] = sfh_feature(url)\n",
    "    features_extracted[13] = submit_to_email_feature(url)\n",
    "    features_extracted[14] = onmouseover_feature(url)\n",
    "    features_extracted[16] = rightclick_feature(url)\n",
    "    features_extracted[15] = iframe_feature(url)\n",
    "    features_extracted[17] = age_of_domain_feature(url)\n",
    "    features_extracted[18] = dns_record_feature(url)\n",
    "    features_extracted[19] = web_traffic_feature(url)\n",
    "    features_extracted[20] = page_rank_feature(url)\n",
    "    \n",
    "    return features_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "elegant-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['having_IP_Address', 'URL_Length', 'Shortining_Service',\n",
      "       'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',\n",
      "       'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',\n",
      "       'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',\n",
      "       'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',\n",
      "       'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',\n",
      "       'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',\n",
      "       'Google_Index', 'Links_pointing_to_page', 'Statistical_report',\n",
      "       'Result'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(complete_training.columns)\n",
    "#7,8,10,17,18,21,27,28,29\n",
    "#SSLfinal_State,Domain_registration_length,port,Abnormal_URL,Redirect,popUpWindow,Google_Index,Links_pointing_to_page,Statistical_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "right-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertEncodingToPositive(data):\n",
    "    mapping = {-1: 2, 0: 0, 1: 1}\n",
    "    i = 0\n",
    "    for col in data:\n",
    "        data[i] = mapping[col]\n",
    "        i+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "handy-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://furniture-shop-vanilla-js.netlify.app/\n",
      "[1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1]\n",
      "[1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.github.com/karans-15\"\n",
    "url = \"https://furniture-shop-vanilla-js.netlify.app/\"\n",
    "features = extract_features(url)\n",
    "print(features)\n",
    "features_extracted = convertEncodingToPositive(features)\n",
    "print(features_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "compact-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "closed-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc = pickle.load(open(\"One_Hot_Encoder\", \"rb\"))\n",
    "transformed_point = one_hot_enc.transform(np.array(features_extracted).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "super-immigration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "large-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"RF_Final_Model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "still-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(transformed_point)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "stainless-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website is SAFE!\n"
     ]
    }
   ],
   "source": [
    "if(prediction==1):\n",
    "    print(\"Website is SAFE!\")\n",
    "elif(prediction==2):\n",
    "    print(\"DANGER!! This appears to be a phishing website\")\n",
    "else:\n",
    "    print(\"Proceed with CAUTION, this seems Suspicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-backup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-history",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-savings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-sender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-timer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-richmond",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-anthony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-samba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-court",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
